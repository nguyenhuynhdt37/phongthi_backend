"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.NaiveJsonOutputParser = void 0;
exports.isModelWithResponseFormat = isModelWithResponseFormat;
exports.isModelWithFormat = isModelWithFormat;
exports.getOutputParserForLLM = getOutputParserForLLM;
exports.executeChain = executeChain;
const output_parsers_1 = require("@langchain/core/output_parsers");
const tracing_1 = require("../../../../utils/tracing");
const promptUtils_1 = require("./promptUtils");
class NaiveJsonOutputParser extends output_parsers_1.JsonOutputParser {
    async parse(text) {
        try {
            const directParsed = JSON.parse(text);
            return directParsed;
        }
        catch (e) {
            return await super.parse(text);
        }
    }
}
exports.NaiveJsonOutputParser = NaiveJsonOutputParser;
function isModelWithResponseFormat(llm) {
    return ('modelKwargs' in llm &&
        !!llm.modelKwargs &&
        typeof llm.modelKwargs === 'object' &&
        'response_format' in llm.modelKwargs);
}
function isModelWithFormat(llm) {
    return 'format' in llm && typeof llm.format !== 'undefined';
}
function getOutputParserForLLM(llm) {
    if (isModelWithResponseFormat(llm) && llm.modelKwargs?.response_format?.type === 'json_object') {
        return new NaiveJsonOutputParser();
    }
    if (isModelWithFormat(llm) && llm.format === 'json') {
        return new NaiveJsonOutputParser();
    }
    return new output_parsers_1.StringOutputParser();
}
async function executeSimpleChain({ context, llm, query, prompt, }) {
    const outputParser = getOutputParserForLLM(llm);
    const chain = prompt.pipe(llm).pipe(outputParser).withConfig((0, tracing_1.getTracingConfig)(context));
    const response = await chain.invoke({
        query,
        signal: context.getExecutionCancelSignal(),
    });
    return [response];
}
async function executeChain({ context, itemIndex, query, llm, outputParser, messages, }) {
    if (!outputParser) {
        const promptTemplate = await (0, promptUtils_1.createPromptTemplate)({
            context,
            itemIndex,
            llm,
            messages,
            query,
        });
        return await executeSimpleChain({
            context,
            llm,
            query,
            prompt: promptTemplate,
        });
    }
    const formatInstructions = outputParser.getFormatInstructions();
    const promptWithInstructions = await (0, promptUtils_1.createPromptTemplate)({
        context,
        itemIndex,
        llm,
        messages,
        formatInstructions,
        query,
    });
    const chain = promptWithInstructions
        .pipe(llm)
        .pipe(outputParser)
        .withConfig((0, tracing_1.getTracingConfig)(context));
    const response = await chain.invoke({ query }, { signal: context.getExecutionCancelSignal() });
    return Array.isArray(response) ? response : [response];
}
//# sourceMappingURL=chainExecutor.js.map