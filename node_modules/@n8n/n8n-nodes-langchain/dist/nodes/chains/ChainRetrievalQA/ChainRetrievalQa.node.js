"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.ChainRetrievalQa = void 0;
const prompts_1 = require("@langchain/core/prompts");
const combine_documents_1 = require("langchain/chains/combine_documents");
const retrieval_1 = require("langchain/chains/retrieval");
const n8n_workflow_1 = require("n8n-workflow");
const descriptions_1 = require("../../../utils/descriptions");
const helpers_1 = require("../../../utils/helpers");
const sharedFields_1 = require("../../../utils/sharedFields");
const tracing_1 = require("../../../utils/tracing");
const SYSTEM_PROMPT_TEMPLATE = `You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
----------------
Context: {context}`;
const LEGACY_INPUT_TEMPLATE_KEY = 'question';
const INPUT_TEMPLATE_KEY = 'input';
const systemPromptOption = {
    displayName: 'System Prompt Template',
    name: 'systemPromptTemplate',
    type: 'string',
    default: SYSTEM_PROMPT_TEMPLATE,
    typeOptions: {
        rows: 6,
    },
};
class ChainRetrievalQa {
    constructor() {
        this.description = {
            displayName: 'Question and Answer Chain',
            name: 'chainRetrievalQa',
            icon: 'fa:link',
            iconColor: 'black',
            group: ['transform'],
            version: [1, 1.1, 1.2, 1.3, 1.4, 1.5],
            description: 'Answer questions about retrieved documents',
            defaults: {
                name: 'Question and Answer Chain',
                color: '#909298',
            },
            codex: {
                alias: ['LangChain'],
                categories: ['AI'],
                subcategories: {
                    AI: ['Chains', 'Root Nodes'],
                },
                resources: {
                    primaryDocumentation: [
                        {
                            url: 'https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainretrievalqa/',
                        },
                    ],
                },
            },
            inputs: [
                "main",
                {
                    displayName: 'Model',
                    maxConnections: 1,
                    type: "ai_languageModel",
                    required: true,
                },
                {
                    displayName: 'Retriever',
                    maxConnections: 1,
                    type: "ai_retriever",
                    required: true,
                },
            ],
            outputs: ["main"],
            credentials: [],
            properties: [
                (0, sharedFields_1.getTemplateNoticeField)(1960),
                {
                    displayName: 'Query',
                    name: 'query',
                    type: 'string',
                    required: true,
                    default: '={{ $json.input }}',
                    displayOptions: {
                        show: {
                            '@version': [1],
                        },
                    },
                },
                {
                    displayName: 'Query',
                    name: 'query',
                    type: 'string',
                    required: true,
                    default: '={{ $json.chat_input }}',
                    displayOptions: {
                        show: {
                            '@version': [1.1],
                        },
                    },
                },
                {
                    displayName: 'Query',
                    name: 'query',
                    type: 'string',
                    required: true,
                    default: '={{ $json.chatInput }}',
                    displayOptions: {
                        show: {
                            '@version': [1.2],
                        },
                    },
                },
                {
                    ...descriptions_1.promptTypeOptions,
                    displayOptions: {
                        hide: {
                            '@version': [{ _cnd: { lte: 1.2 } }],
                        },
                    },
                },
                {
                    ...descriptions_1.textFromPreviousNode,
                    displayOptions: { show: { promptType: ['auto'], '@version': [{ _cnd: { gte: 1.4 } }] } },
                },
                {
                    displayName: 'Prompt (User Message)',
                    name: 'text',
                    type: 'string',
                    required: true,
                    default: '',
                    placeholder: 'e.g. Hello, how can you help me?',
                    typeOptions: {
                        rows: 2,
                    },
                    displayOptions: {
                        show: {
                            promptType: ['define'],
                        },
                    },
                },
                {
                    displayName: 'Options',
                    name: 'options',
                    type: 'collection',
                    default: {},
                    placeholder: 'Add Option',
                    options: [
                        {
                            ...systemPromptOption,
                            description: `Template string used for the system prompt. This should include the variable \`{context}\` for the provided context. For text completion models, you should also include the variable \`{${LEGACY_INPUT_TEMPLATE_KEY}}\` for the user’s query.`,
                            displayOptions: {
                                show: {
                                    '@version': [{ _cnd: { lt: 1.5 } }],
                                },
                            },
                        },
                        {
                            ...systemPromptOption,
                            description: `Template string used for the system prompt. This should include the variable \`{context}\` for the provided context. For text completion models, you should also include the variable \`{${INPUT_TEMPLATE_KEY}}\` for the user’s query.`,
                            displayOptions: {
                                show: {
                                    '@version': [{ _cnd: { gte: 1.5 } }],
                                },
                            },
                        },
                    ],
                },
            ],
        };
    }
    async execute() {
        this.logger.debug('Executing Retrieval QA Chain');
        const items = this.getInputData();
        const returnData = [];
        for (let itemIndex = 0; itemIndex < items.length; itemIndex++) {
            try {
                const model = (await this.getInputConnectionData("ai_languageModel", 0));
                const retriever = (await this.getInputConnectionData("ai_retriever", 0));
                let query;
                if (this.getNode().typeVersion <= 1.2) {
                    query = this.getNodeParameter('query', itemIndex);
                }
                else {
                    query = (0, helpers_1.getPromptInputByType)({
                        ctx: this,
                        i: itemIndex,
                        inputKey: 'text',
                        promptTypeKey: 'promptType',
                    });
                }
                if (query === undefined) {
                    throw new n8n_workflow_1.NodeOperationError(this.getNode(), 'The ‘query‘ parameter is empty.');
                }
                const options = this.getNodeParameter('options', itemIndex, {});
                let templateText = options.systemPromptTemplate ?? SYSTEM_PROMPT_TEMPLATE;
                if (this.getNode().typeVersion < 1.5) {
                    templateText = templateText.replace(`{${LEGACY_INPUT_TEMPLATE_KEY}}`, `{${INPUT_TEMPLATE_KEY}}`);
                }
                let promptTemplate;
                if ((0, helpers_1.isChatInstance)(model)) {
                    const messages = [
                        prompts_1.SystemMessagePromptTemplate.fromTemplate(templateText),
                        prompts_1.HumanMessagePromptTemplate.fromTemplate('{input}'),
                    ];
                    promptTemplate = prompts_1.ChatPromptTemplate.fromMessages(messages);
                }
                else {
                    const questionSuffix = options.systemPromptTemplate === undefined ? '\n\nQuestion: {input}\nAnswer:' : '';
                    promptTemplate = new prompts_1.PromptTemplate({
                        template: templateText + questionSuffix,
                        inputVariables: ['context', 'input'],
                    });
                }
                const combineDocsChain = await (0, combine_documents_1.createStuffDocumentsChain)({
                    llm: model,
                    prompt: promptTemplate,
                });
                const retrievalChain = await (0, retrieval_1.createRetrievalChain)({
                    combineDocsChain,
                    retriever,
                });
                const tracingConfig = (0, tracing_1.getTracingConfig)(this);
                const response = await retrievalChain
                    .withConfig(tracingConfig)
                    .invoke({ input: query }, { signal: this.getExecutionCancelSignal() });
                const answer = response.answer;
                if (this.getNode().typeVersion >= 1.5) {
                    returnData.push({ json: { response: answer } });
                }
                else {
                    returnData.push({ json: { response: { text: answer } } });
                }
            }
            catch (error) {
                if (this.continueOnFail()) {
                    const metadata = (0, n8n_workflow_1.parseErrorMetadata)(error);
                    returnData.push({
                        json: { error: error.message },
                        pairedItem: { item: itemIndex },
                        metadata,
                    });
                    continue;
                }
                throw error;
            }
        }
        return [returnData];
    }
}
exports.ChainRetrievalQa = ChainRetrievalQa;
//# sourceMappingURL=ChainRetrievalQa.node.js.map