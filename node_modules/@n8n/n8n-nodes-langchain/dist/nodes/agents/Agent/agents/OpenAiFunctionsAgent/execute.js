"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.openAiFunctionsAgentExecute = openAiFunctionsAgentExecute;
const prompts_1 = require("@langchain/core/prompts");
const openai_1 = require("@langchain/openai");
const agents_1 = require("langchain/agents");
const memory_1 = require("langchain/memory");
const n8n_workflow_1 = require("n8n-workflow");
const helpers_1 = require("../../../../../utils/helpers");
const N8nOutputParser_1 = require("../../../../../utils/output_parsers/N8nOutputParser");
const tracing_1 = require("../../../../../utils/tracing");
const utils_1 = require("../utils");
async function openAiFunctionsAgentExecute(nodeVersion) {
    this.logger.debug('Executing OpenAi Functions Agent');
    const model = (await this.getInputConnectionData("ai_languageModel", 0));
    if (!(model instanceof openai_1.ChatOpenAI)) {
        throw new n8n_workflow_1.NodeOperationError(this.getNode(), 'OpenAI Functions Agent requires OpenAI Chat Model');
    }
    const memory = (await this.getInputConnectionData("ai_memory", 0));
    const tools = await (0, helpers_1.getConnectedTools)(this, nodeVersion >= 1.5, false);
    const outputParser = await (0, N8nOutputParser_1.getOptionalOutputParser)(this);
    const options = this.getNodeParameter('options', 0, {});
    const agentConfig = {
        tags: ['openai-functions'],
        agent: agents_1.OpenAIAgent.fromLLMAndTools(model, tools, {
            prefix: options.systemMessage,
        }),
        tools,
        maxIterations: options.maxIterations ?? 10,
        returnIntermediateSteps: options?.returnIntermediateSteps === true,
        memory: memory ??
            new memory_1.BufferMemory({
                returnMessages: true,
                memoryKey: 'chat_history',
                inputKey: 'input',
                outputKey: 'output',
            }),
    };
    const agentExecutor = agents_1.AgentExecutor.fromAgentAndTools(agentConfig);
    const returnData = [];
    let prompt;
    if (outputParser) {
        const formatInstructions = outputParser.getFormatInstructions();
        prompt = new prompts_1.PromptTemplate({
            template: '{input}\n{formatInstructions}',
            inputVariables: ['input'],
            partialVariables: { formatInstructions },
        });
    }
    const items = this.getInputData();
    for (let itemIndex = 0; itemIndex < items.length; itemIndex++) {
        try {
            let input;
            if (this.getNode().typeVersion <= 1.2) {
                input = this.getNodeParameter('text', itemIndex);
            }
            else {
                input = (0, helpers_1.getPromptInputByType)({
                    ctx: this,
                    i: itemIndex,
                    inputKey: 'text',
                    promptTypeKey: 'promptType',
                });
            }
            if (input === undefined) {
                throw new n8n_workflow_1.NodeOperationError(this.getNode(), 'The ‘text‘ parameter is empty.');
            }
            if (prompt) {
                input = (await prompt.invoke({ input })).value;
            }
            const response = await agentExecutor
                .withConfig((0, tracing_1.getTracingConfig)(this))
                .invoke({ input, outputParser });
            if (outputParser) {
                response.output = await (0, utils_1.extractParsedOutput)(this, outputParser, response.output);
            }
            returnData.push({ json: response });
        }
        catch (error) {
            if (this.continueOnFail()) {
                returnData.push({ json: { error: error.message }, pairedItem: { item: itemIndex } });
                continue;
            }
            throw error;
        }
    }
    return [returnData];
}
//# sourceMappingURL=execute.js.map